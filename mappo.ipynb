{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPPO Integration\n",
    "\n",
    "This runs the integrated MAPPO algorithm with the patrolling zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import onpolicy.runner.shared.patrolling_runner as patrolling_runner\n",
    "from onpolicy.scripts.train.train_patrolling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args, parser):\n",
    "    parser.add_argument(\"--graph_name\", type=str,\n",
    "                        default=\"cumberland\", \n",
    "                        help=\"which graph to run on.\")\n",
    "    parser.add_argument(\"--num_agents\", type=int, default=3,\n",
    "                        help=\"number of controlled players.\")\n",
    "    parser.add_argument(\"--representation\", type=str, default=\"simple115v2\", \n",
    "                        choices=[\"simple115v2\", \"extracted\", \"pixels_gray\", \n",
    "                                 \"pixels\"],\n",
    "                        help=\"representation used to build the observation.\")\n",
    "    parser.add_argument(\"--rewards\", type=str, default=\"scoring\", \n",
    "                        help=\"comma separated list of rewards to be added.\")\n",
    "    parser.add_argument(\"--smm_width\", type=int, default=96,\n",
    "                        help=\"width of super minimap.\")\n",
    "    parser.add_argument(\"--smm_height\", type=int, default=72,\n",
    "                        help=\"height of super minimap.\")\n",
    "    parser.add_argument(\"--remove_redundancy\", action=\"store_true\", \n",
    "                        default=False, \n",
    "                        help=\"by default False. If True, remove redundancy features\")\n",
    "    parser.add_argument(\"--zero_feature\", action=\"store_true\", \n",
    "                        default=False, \n",
    "                        help=\"by default False. If True, replace -1 by 0\")\n",
    "    parser.add_argument(\"--eval_deterministic\", action=\"store_false\", \n",
    "                        default=True, \n",
    "                        help=\"by default True. If False, sample action according to probability\")\n",
    "    parser.add_argument(\"--share_reward\", action='store_false', \n",
    "                        default=True, \n",
    "                        help=\"by default true. If false, use different reward for each agent.\")\n",
    "\n",
    "    parser.add_argument(\"--save_videos\", action=\"store_true\", default=False, \n",
    "                        help=\"by default, do not save render video. If set, save video.\")\n",
    "    parser.add_argument(\"--video_dir\", type=str, default=\"\", \n",
    "                        help=\"directory to save videos.\")\n",
    "                        \n",
    "    all_args = parser.parse_known_args(args)[0]\n",
    "\n",
    "    return all_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose to use gpu...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DiagGaussian.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39monpolicy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrunner\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseparated\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfootball_runner\u001b[39;00m \u001b[39mimport\u001b[39;00m FootballRunner \u001b[39mas\u001b[39;00m Runner\n\u001b[0;32m     95\u001b[0m runner \u001b[39m=\u001b[39m Runner(config)\n\u001b[1;32m---> 96\u001b[0m runner\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m     98\u001b[0m \u001b[39m# post process\u001b[39;00m\n\u001b[0;32m     99\u001b[0m envs\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\Development\\patrolling_zoo2\\onpolicy\\runner\\shared\\patrolling_runner.py:35\u001b[0m, in \u001b[0;36mPatrollingRunner.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mlr_decay(episode, episodes)\n\u001b[0;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode_length):\n\u001b[0;32m     34\u001b[0m     \u001b[39m# Sample actions\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     values, actions, action_log_probs, rnn_states, rnn_states_critic, actions_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect(step)\n\u001b[0;32m     37\u001b[0m     \u001b[39m# Obser reward and next obs\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     obs, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs\u001b[39m.\u001b[39mstep(actions_env)\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\miniconda3\\envs\\patrolling_zoo2\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\Development\\patrolling_zoo2\\onpolicy\\runner\\shared\\patrolling_runner.py:92\u001b[0m, in \u001b[0;36mPatrollingRunner.collect\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprep_rollout()\n\u001b[0;32m     91\u001b[0m \u001b[39m# [n_envs, n_agents, ...] -> [n_envs*n_agents, ...]\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m values, actions, action_log_probs, rnn_states, rnn_states_critic \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mget_actions(\n\u001b[0;32m     93\u001b[0m     np\u001b[39m.\u001b[39;49mconcatenate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer\u001b[39m.\u001b[39;49mshare_obs[step]),\n\u001b[0;32m     94\u001b[0m     np\u001b[39m.\u001b[39;49mconcatenate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer\u001b[39m.\u001b[39;49mobs[step]),\n\u001b[0;32m     95\u001b[0m     np\u001b[39m.\u001b[39;49mconcatenate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer\u001b[39m.\u001b[39;49mrnn_states[step]),\n\u001b[0;32m     96\u001b[0m     np\u001b[39m.\u001b[39;49mconcatenate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer\u001b[39m.\u001b[39;49mrnn_states_critic[step]),\n\u001b[0;32m     97\u001b[0m     np\u001b[39m.\u001b[39;49mconcatenate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer\u001b[39m.\u001b[39;49mmasks[step])\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[39m# [n_envs*n_agents, ...] -> [n_envs, n_agents, ...]\u001b[39;00m\n\u001b[0;32m    101\u001b[0m values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39msplit(_t2n(values), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_rollout_threads))\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\Development\\patrolling_zoo2\\onpolicy\\algorithms\\r_mappo\\algorithm\\rMAPPOPolicy.py:67\u001b[0m, in \u001b[0;36mR_MAPPOPolicy.get_actions\u001b[1;34m(self, cent_obs, obs, rnn_states_actor, rnn_states_critic, masks, available_actions, deterministic)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_actions\u001b[39m(\u001b[39mself\u001b[39m, cent_obs, obs, rnn_states_actor, rnn_states_critic, masks, available_actions\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m                 deterministic\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     50\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m    Compute actions and value function predictions for the given inputs.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m    :param cent_obs (np.ndarray): centralized input to the critic.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m    :return rnn_states_critic: (torch.Tensor) updated critic network RNN states.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     actions, action_log_probs, rnn_states_actor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor(obs,\n\u001b[0;32m     68\u001b[0m                                                              rnn_states_actor,\n\u001b[0;32m     69\u001b[0m                                                              masks,\n\u001b[0;32m     70\u001b[0m                                                              available_actions,\n\u001b[0;32m     71\u001b[0m                                                              deterministic)\n\u001b[0;32m     73\u001b[0m     values, rnn_states_critic \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic(cent_obs, rnn_states_critic, masks)\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m values, actions, action_log_probs, rnn_states_actor, rnn_states_critic\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\miniconda3\\envs\\patrolling_zoo2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\Development\\patrolling_zoo2\\onpolicy\\algorithms\\r_mappo\\algorithm\\r_actor_critic.py:68\u001b[0m, in \u001b[0;36mR_Actor.forward\u001b[1;34m(self, obs, rnn_states, masks, available_actions, deterministic)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_naive_recurrent_policy \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_recurrent_policy:\n\u001b[0;32m     66\u001b[0m     actor_features, rnn_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(actor_features, rnn_states, masks)\n\u001b[1;32m---> 68\u001b[0m actions, action_log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(actor_features, available_actions, deterministic)\n\u001b[0;32m     70\u001b[0m \u001b[39mreturn\u001b[39;00m actions, action_log_probs, rnn_states\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\miniconda3\\envs\\patrolling_zoo2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\Development\\patrolling_zoo2\\onpolicy\\algorithms\\utils\\act.py:79\u001b[0m, in \u001b[0;36mACTLayer.forward\u001b[1;34m(self, x, available_actions, deterministic)\u001b[0m\n\u001b[0;32m     76\u001b[0m     action_log_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(action_log_probs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     action_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_out(x, available_actions)\n\u001b[0;32m     80\u001b[0m     actions \u001b[39m=\u001b[39m action_logits\u001b[39m.\u001b[39mmode() \u001b[39mif\u001b[39;00m deterministic \u001b[39melse\u001b[39;00m action_logits\u001b[39m.\u001b[39msample() \n\u001b[0;32m     81\u001b[0m     action_log_probs \u001b[39m=\u001b[39m action_logits\u001b[39m.\u001b[39mlog_probs(actions)\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\miniconda3\\envs\\patrolling_zoo2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: DiagGaussian.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "parser = get_config()\n",
    "all_args = parse_args([], parser)\n",
    "\n",
    "all_args.env_name = \"Patrolling\"\n",
    "all_args.graph_file = f\"patrolling_zoo/env/{all_args.graph_name}.graph\"\n",
    "\n",
    "all_args.algorithm_name = \"rmappo\"\n",
    "all_args.use_recurrent_policy = True\n",
    "all_args.use_naive_recurrent_policy = False\n",
    "all_args.use_centralized_V = True\n",
    "\n",
    "all_args.n_rollout_threads = 1\n",
    "\n",
    "all_args.use_wandb = False\n",
    "\n",
    "# cuda\n",
    "if all_args.cuda and torch.cuda.is_available():\n",
    "    print(\"choose to use gpu...\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.set_num_threads(all_args.n_training_threads)\n",
    "    if all_args.cuda_deterministic:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "else:\n",
    "    print(\"choose to use cpu...\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_num_threads(all_args.n_training_threads)\n",
    "\n",
    "# run dir\n",
    "run_dir = Path(os.path.join(\".\", \"results\", all_args.env_name, all_args.graph_name, all_args.algorithm_name, all_args.experiment_name))\n",
    "if not run_dir.exists():\n",
    "    os.makedirs(str(run_dir))\n",
    "\n",
    "# wandb\n",
    "if all_args.use_wandb:\n",
    "    run = wandb.init(config=all_args,\n",
    "                        project=all_args.env_name,\n",
    "                        entity=all_args.user_name,\n",
    "                        notes=socket.gethostname(),\n",
    "                        name=\"-\".join([\n",
    "                        all_args.algorithm_name,\n",
    "                        all_args.experiment_name,\n",
    "                        \"seed\" + str(all_args.seed)\n",
    "                        ]),\n",
    "                        group=all_args.graph_name,\n",
    "                        dir=str(run_dir),\n",
    "                        job_type=\"training\",\n",
    "                        reinit=True)\n",
    "else:\n",
    "    if not run_dir.exists():\n",
    "        curr_run = 'run1'\n",
    "    else:\n",
    "        exst_run_nums = [int(str(folder.name).split('run')[1]) for folder in run_dir.iterdir() if str(folder.name).startswith('run')]\n",
    "        if len(exst_run_nums) == 0:\n",
    "            curr_run = 'run1'\n",
    "        else:\n",
    "            curr_run = 'run%i' % (max(exst_run_nums) + 1)\n",
    "    run_dir = run_dir / curr_run\n",
    "    if not run_dir.exists():\n",
    "        os.makedirs(str(run_dir))\n",
    "\n",
    "setproctitle.setproctitle(\"-\".join([\n",
    "    all_args.env_name, \n",
    "    all_args.graph_name, \n",
    "    all_args.algorithm_name, \n",
    "    all_args.experiment_name\n",
    "]) + \"@\" + all_args.user_name)\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(all_args.seed)\n",
    "torch.cuda.manual_seed_all(all_args.seed)\n",
    "np.random.seed(all_args.seed)\n",
    "\n",
    "# env init\n",
    "envs = make_train_env(all_args)\n",
    "eval_envs = make_eval_env(all_args) if all_args.use_eval else None\n",
    "num_agents = all_args.num_agents\n",
    "\n",
    "config = {\n",
    "    \"all_args\": all_args,\n",
    "    \"envs\": envs,\n",
    "    \"eval_envs\": eval_envs,\n",
    "    \"num_agents\": num_agents,\n",
    "    \"device\": device,\n",
    "    \"run_dir\": run_dir\n",
    "}\n",
    "\n",
    "# run experiments\n",
    "if all_args.share_policy:\n",
    "    from onpolicy.runner.shared.patrolling_runner import PatrollingRunner as Runner\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    from onpolicy.runner.separated.football_runner import FootballRunner as Runner\n",
    "\n",
    "runner = Runner(config)\n",
    "runner.run()\n",
    "\n",
    "# post process\n",
    "envs.close()\n",
    "if all_args.use_eval and eval_envs is not envs:\n",
    "    eval_envs.close()\n",
    "\n",
    "if all_args.use_wandb:\n",
    "    run.finish()\n",
    "else:\n",
    "    runner.writter.export_scalars_to_json(str(runner.log_dir + '/summary.json'))\n",
    "    runner.writter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patrolling_zoo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
