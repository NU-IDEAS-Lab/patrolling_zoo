{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPPO Integration\n",
    "\n",
    "This runs the integrated MAPPO algorithm with the patrolling zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from onpolicy.scripts.train.train_patrolling import get_config, parse_args, main\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_config()\n",
    "all_args = parse_args([], parser)\n",
    "\n",
    "all_args.experiment_name = \"sepBuffersSkipAsyncAmadmGAE\"\n",
    "all_args.env_name = \"Patrolling\"\n",
    "all_args.user_name = \"anthony-goeckner\"\n",
    "\n",
    "all_args.num_agents = 6\n",
    "all_args.agent_speed = 40.0\n",
    "all_args.observe_method = \"adjacency\"\n",
    "\n",
    "all_args.graph_name = \"cumberland\"\n",
    "all_args.graph_file = f\"patrolling_zoo/env/{all_args.graph_name}.graph\"\n",
    "all_args.num_env_steps = 10e5 * 3 #total number of steps\n",
    "all_args.episode_length = 800 #number of steps in a training episode\n",
    "all_args.max_cycles = all_args.episode_length #number of steps in an environment episode\n",
    "\n",
    "all_args.algorithm_name = \"rmappo\"\n",
    "all_args.use_recurrent_policy = True\n",
    "all_args.use_naive_recurrent_policy = False\n",
    "all_args.use_centralized_V = True\n",
    "all_args.use_gae = False\n",
    "all_args.use_gae_amadm = True\n",
    "all_args.share_policy = False\n",
    "all_args.skip_steps_sync = False\n",
    "all_args.skip_steps_async = True\n",
    "\n",
    "all_args.n_rollout_threads = 100\n",
    "all_args.cuda = True\n",
    "all_args.cuda_idx = 5\n",
    "\n",
    "all_args.use_wandb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u are choosing to use rmappo, we set use_recurrent_policy to be True\n",
      "choose to use gpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manthony-goeckner\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anthony/dev/patrolling_zoo3/onpolicy/scripts/results/Patrolling/cumberland/rmappo/sepBuffersSkipAsyncAmadmGAE/wandb/run-20230831_234146-byj63bqf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anthony-goeckner/Patrolling/runs/byj63bqf' target=\"_blank\">rmappo-sepBuffersSkipAsyncAmadmGAE-20230831-234145-seed1</a></strong> to <a href='https://wandb.ai/anthony-goeckner/Patrolling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anthony-goeckner/Patrolling' target=\"_blank\">https://wandb.ai/anthony-goeckner/Patrolling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anthony-goeckner/Patrolling/runs/byj63bqf' target=\"_blank\">https://wandb.ai/anthony-goeckner/Patrolling/runs/byj63bqf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 0/37 episodes, total num timesteps 80000/3000000.0, FPS 224.\n",
      "\n",
      "average episode rewards is 90183.20922851562\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 1/37 episodes, total num timesteps 160000/3000000.0, FPS 220.\n",
      "\n",
      "average episode rewards is 147394.90966796875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 2/37 episodes, total num timesteps 240000/3000000.0, FPS 217.\n",
      "\n",
      "average episode rewards is 157327.34375\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 3/37 episodes, total num timesteps 320000/3000000.0, FPS 215.\n",
      "\n",
      "average episode rewards is 149378.515625\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 4/37 episodes, total num timesteps 400000/3000000.0, FPS 214.\n",
      "\n",
      "average episode rewards is 217324.4873046875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 5/37 episodes, total num timesteps 480000/3000000.0, FPS 212.\n",
      "\n",
      "average episode rewards is 204372.216796875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 6/37 episodes, total num timesteps 560000/3000000.0, FPS 212.\n",
      "\n",
      "average episode rewards is 205639.9658203125\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 7/37 episodes, total num timesteps 640000/3000000.0, FPS 211.\n",
      "\n",
      "average episode rewards is 227810.9619140625\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 8/37 episodes, total num timesteps 720000/3000000.0, FPS 209.\n",
      "\n",
      "average episode rewards is 245333.10546875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 9/37 episodes, total num timesteps 800000/3000000.0, FPS 210.\n",
      "\n",
      "average episode rewards is 207070.9228515625\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 10/37 episodes, total num timesteps 880000/3000000.0, FPS 209.\n",
      "\n",
      "average episode rewards is 197863.63525390625\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 11/37 episodes, total num timesteps 960000/3000000.0, FPS 209.\n",
      "\n",
      "average episode rewards is 170056.87255859375\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 12/37 episodes, total num timesteps 1040000/3000000.0, FPS 209.\n",
      "\n",
      "average episode rewards is 185159.55810546875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 13/37 episodes, total num timesteps 1120000/3000000.0, FPS 209.\n",
      "\n",
      "average episode rewards is 177060.09521484375\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 14/37 episodes, total num timesteps 1200000/3000000.0, FPS 209.\n",
      "\n",
      "average episode rewards is 162768.24951171875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 15/37 episodes, total num timesteps 1280000/3000000.0, FPS 208.\n",
      "\n",
      "average episode rewards is 190090.19775390625\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 16/37 episodes, total num timesteps 1360000/3000000.0, FPS 208.\n",
      "\n",
      "average episode rewards is 231695.01953125\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 17/37 episodes, total num timesteps 1440000/3000000.0, FPS 208.\n",
      "\n",
      "average episode rewards is 260048.1689453125\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 18/37 episodes, total num timesteps 1520000/3000000.0, FPS 208.\n",
      "\n",
      "average episode rewards is 295914.5751953125\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 19/37 episodes, total num timesteps 1600000/3000000.0, FPS 208.\n",
      "\n",
      "average episode rewards is 311883.49609375\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 20/37 episodes, total num timesteps 1680000/3000000.0, FPS 208.\n",
      "\n",
      "average episode rewards is 273980.517578125\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 21/37 episodes, total num timesteps 1760000/3000000.0, FPS 207.\n",
      "\n",
      "average episode rewards is 305148.4375\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 22/37 episodes, total num timesteps 1840000/3000000.0, FPS 207.\n",
      "\n",
      "average episode rewards is 265769.6044921875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 23/37 episodes, total num timesteps 1920000/3000000.0, FPS 207.\n",
      "\n",
      "average episode rewards is 245348.974609375\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 24/37 episodes, total num timesteps 2000000/3000000.0, FPS 206.\n",
      "\n",
      "average episode rewards is 246582.8857421875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 25/37 episodes, total num timesteps 2080000/3000000.0, FPS 206.\n",
      "\n",
      "average episode rewards is 267795.263671875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 26/37 episodes, total num timesteps 2160000/3000000.0, FPS 206.\n",
      "\n",
      "average episode rewards is 298418.2861328125\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 27/37 episodes, total num timesteps 2240000/3000000.0, FPS 206.\n",
      "\n",
      "average episode rewards is 273456.7138671875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 28/37 episodes, total num timesteps 2320000/3000000.0, FPS 205.\n",
      "\n",
      "average episode rewards is 260932.861328125\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 29/37 episodes, total num timesteps 2400000/3000000.0, FPS 205.\n",
      "\n",
      "average episode rewards is 224511.4013671875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 30/37 episodes, total num timesteps 2480000/3000000.0, FPS 205.\n",
      "\n",
      "average episode rewards is 225611.0595703125\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 31/37 episodes, total num timesteps 2560000/3000000.0, FPS 204.\n",
      "\n",
      "average episode rewards is 258568.84765625\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 32/37 episodes, total num timesteps 2640000/3000000.0, FPS 204.\n",
      "\n",
      "average episode rewards is 213731.884765625\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 33/37 episodes, total num timesteps 2720000/3000000.0, FPS 204.\n",
      "\n",
      "average episode rewards is 218387.060546875\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 34/37 episodes, total num timesteps 2800000/3000000.0, FPS 204.\n",
      "\n",
      "average episode rewards is 239530.37109375\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 35/37 episodes, total num timesteps 2880000/3000000.0, FPS 204.\n",
      "\n",
      "average episode rewards is 223449.31640625\n",
      "\n",
      " Env Patrolling Algo rmappo Exp sepBuffersSkipAsyncAmadmGAE updates 36/37 episodes, total num timesteps 2960000/3000000.0, FPS 203.\n",
      "\n",
      "average episode rewards is 261068.5302734375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>agent0/actor_grad_norm</td><td>█▄▃▂▃▂▄▃▂▃▂▂▁▂▁▂▃▄▃▁▂▂▃▃▄▂▂▂▁▂▃▂▃▃▆▂▃</td></tr><tr><td>agent0/critic_grad_norm</td><td>▁▄▂▂█▂▆▃▄▂▅▄▄▁▂▁▃▂▁▃▂▁▃▁▁▃▁▂▂▂▁▁▁▂▁▁▁</td></tr><tr><td>agent0/dist_entropy</td><td>▇▆▅▇▆▅▆▇▇▇▇▆▇▇▆▇▇▇▆▆▃▁▃▆█▇▇▇▇▆▆▆▆▅▃▂▄</td></tr><tr><td>agent0/policy_loss</td><td>▂▅▆▅▇▆▁▄▅▆▃▇▆▆▃▇▅▃█▄▃▆▄▅█▄▆▅▆▃▄▅▆▄▄▆▆</td></tr><tr><td>agent0/ratio</td><td>▂▂▃▄▃▃▄▄▄▂▁▁▂▃▃▄▅▅▃▅▆▆▆▆▄▆▇▆▆▆▇▆▅▇█▇▅</td></tr><tr><td>agent0/value_loss</td><td>▆█▃▂▇▁▅▃▂▁▅▁▂▃▄▄▅▇▄▆▆▃▄▂▂▅▃▄▃▂▄▂▂▂▂▃▃</td></tr><tr><td>agent1/actor_grad_norm</td><td>█▄▂▂▃▂▂▁▂▁▂▂▂▂▃▂▁▂▂▂▂▂▃▂▃▂▃▂▃▃▃▁▃▂▂▂▂</td></tr><tr><td>agent1/critic_grad_norm</td><td>▂▂▂▂▆▂▃▆▆▅█▁▄▂▄▃▁▄▄▂▄▁▁▂▁▂▁▂▁▂▁▃▁▁▁▁▁</td></tr><tr><td>agent1/dist_entropy</td><td>▆▇▇▇▇███▆▆▆▆▄▄▃▃▂▂▂▂▁▆▇▆▆▆▅▄▆▅▅▄▄▂▂▁▁</td></tr><tr><td>agent1/policy_loss</td><td>▁▁▇▄▅▇▆▄▆▅▄▆▄▂▆▅▆▄▇▅▅▅▆▅▅▆▃▄▂▂▆▅█▅▄▄▄</td></tr><tr><td>agent1/ratio</td><td>▃▅▄▂▁▂▃▃▁▂▁▁▁▃▁▂▂▂▂▃▃▃▃▄▃▃▃▄█▇▆▇▅█▆▅▅</td></tr><tr><td>agent1/value_loss</td><td>▆█▂▃▇▂▆▄▃▃▅▁▃▄▂▃▃▆▄▆▅▄▄▁▂▄▃▄▃▁▃▃▂▂▂▂▃</td></tr><tr><td>agent2/actor_grad_norm</td><td>█▄▅▃▃▃▂▃▂▁▁▂▃▁▃▁▂▂▁▂▂▃▁▂▄▂▂▂▃▄▄▂▃▃▂▂▄</td></tr><tr><td>agent2/critic_grad_norm</td><td>▂▇▃▂▂▃█▄▁▆▄▁▁▇▁▃█▃▃▆▂▅▇▆▂▆▂▃▇▃▅▁▂▁▅▄▄</td></tr><tr><td>agent2/dist_entropy</td><td>▄▁▆▇▄▆▅▅▇▆▅▆▇██▇▆▆█▇▅▆▅▁▅▇▆▅▆▆▄▆▆▅▂▃▁</td></tr><tr><td>agent2/policy_loss</td><td>▅▄▇▅▆▇▆█▇▆▃▇█▆▅▆▆▄▅▂▅▄▅▄█▁▄▂▅▅▆▃▇▆▄▄▅</td></tr><tr><td>agent2/ratio</td><td>▂▁▂▂▄▃▃▃▂▄▅▅▄▃▃▄▄▅▄▆▃▅▅▆▅▆▆█▄▅▅▆▄▄▃▄▃</td></tr><tr><td>agent2/value_loss</td><td>▇█▂▂▇▂▅▅▃▂▆▃▄▅▃▄▅█▅▆▅▄▆▃▃▅▃▅▄▂▄▄▁▂▂▂▃</td></tr><tr><td>agent3/actor_grad_norm</td><td>█▃▃▁▁▁▃▂▁▂▂▁▁▂▂▂▂▃▂▂▂▃▂▂▃▂▂▃▃▃▂▂▃▄▂▃▃</td></tr><tr><td>agent3/critic_grad_norm</td><td>▁█▃▇▂▂▃▂▁▂▄▁▂▁▂▂▃▇▆▂▇▂▂▁▁▃▁▅▂▂▁▁▅▁▁▁▁</td></tr><tr><td>agent3/dist_entropy</td><td>▆▇█▇█▇▅▇▆▆▇▇▄▄▇▆▅▃▁▄▅▆▅▇▄▆▅▅▄▆▅▅▄▇▆▅▄</td></tr><tr><td>agent3/policy_loss</td><td>▅▅▇▆▄▇▄▅▅█▄▇▆▆▃▂▃▂▅▂▄▇▆█▅▄█▅▆▅▅▁▇▃▃▂▁</td></tr><tr><td>agent3/ratio</td><td>▂▁▄▄▅▅▅▅▅▄▄▅▅▇▇▇█▇▅▆▆▅▅▅▇▆▆▇▆▆▅▅▅▅▇▇▆</td></tr><tr><td>agent3/value_loss</td><td>▇█▁▂█▂▆▄▃▂▆▃▄▄▄▅▇█▅▆▇▅▅▁▂▆▃▅▅▁▄▃▂▂▃▃▄</td></tr><tr><td>agent4/actor_grad_norm</td><td>█▄▃▂▁▁▂▂▂▂▁▂▂▃▂▂▃▂▄▂▂▂▂▂▂▂▃▂▄▄▂▂▃▃▂▁▂</td></tr><tr><td>agent4/critic_grad_norm</td><td>▁▃▁▂▅▁▄▄▁▂▂▄▄█▂▂▂▂▂▅▂▁▄▁▃▄▃▂▄▃▃▁▂▃▁▂▁</td></tr><tr><td>agent4/dist_entropy</td><td>▇▃▅▆▇▆▆▃▅▆▆▅▆▂▇▇█▇▆▇▇▆▆▅▅▂▁▁▁▂▆▆▇█▆▄▄</td></tr><tr><td>agent4/policy_loss</td><td>▃▂▆▅▃▆▆▇▇▆▅█▆▇▅▅▄▇▆▄▆▇▇▆▄▄▅▃▃▁▆▆▄▅▅▇▅</td></tr><tr><td>agent4/ratio</td><td>▁▅▅▅▆▆▆▆▅▇▇▆▅▆▅▆▅▄▅▅▅▆▅▅▆▅▅▆▇█▆▆▇▆▇▆▇</td></tr><tr><td>agent4/value_loss</td><td>▆▇▂▄█▃▆▅▃▄▆▄▃▅▅▄▅▇▅▇▆▄▆▂▁▄▂▅▄▂▂▂▁▂▂▃▃</td></tr><tr><td>agent5/actor_grad_norm</td><td>█▃▂▃▁▂▂▁▁▁▂▂▂▂▂▂▂▂▂▁▁▃▂▂▁▂▂▂▂▂▂▂▃▃▃▂▃</td></tr><tr><td>agent5/critic_grad_norm</td><td>▁▄▂▂▄▂▂▃▇▁▆▂▃▇▇▇██▂▂▂▆▂▁▃▇▃▆▁▁▁▂▁▃▂▅▅</td></tr><tr><td>agent5/dist_entropy</td><td>▆▅▅▆▂▄▇▅▇█▇▅▆▅█▇▆▇▆▆▆▆▅▆▅▅▆█▇▄▃▅▃▂▁▃▄</td></tr><tr><td>agent5/policy_loss</td><td>▃▅▄▂▄▇▄▂▄▅▄▄▆▃▅▆▄▃▆▄▇▆▃▅▄▃▆█▃▄▁▄▅▄▆▄▅</td></tr><tr><td>agent5/ratio</td><td>▁▄▅▇█▇▇█▅▄▅▄▄▅▅▃▅▆▇▅▅▅▅▃▄▄▄▃▆▄▅▅▅▆▄▅▄</td></tr><tr><td>agent5/value_loss</td><td>▆▇▁▃█▂▆▅▅▃▅▄▅▆▄▅▆█▆▇▆▄▅▃▃▆▅▅▅▂▃▃▃▁▂▃▄</td></tr><tr><td>average_episode_rewards</td><td>▁▃▃▃▅▅▅▅▆▅▄▄▄▄▃▄▅▆▇█▇█▇▆▆▇█▇▆▅▅▆▅▅▆▅▆</td></tr><tr><td>avg_idleness</td><td>▄▆▅▄▂▄▅▄▅▄▄▅▄▄▂▄▅▅▂▄▂▂▄▂▆▇▄▅█▁█▆▆▂▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>agent0/actor_grad_norm</td><td>0.06249</td></tr><tr><td>agent0/critic_grad_norm</td><td>0.00831</td></tr><tr><td>agent0/dist_entropy</td><td>3.65453</td></tr><tr><td>agent0/policy_loss</td><td>-0.05677</td></tr><tr><td>agent0/ratio</td><td>0.142</td></tr><tr><td>agent0/value_loss</td><td>0.52937</td></tr><tr><td>agent1/actor_grad_norm</td><td>0.04522</td></tr><tr><td>agent1/critic_grad_norm</td><td>0.00781</td></tr><tr><td>agent1/dist_entropy</td><td>3.64144</td></tr><tr><td>agent1/policy_loss</td><td>-0.06202</td></tr><tr><td>agent1/ratio</td><td>0.13208</td></tr><tr><td>agent1/value_loss</td><td>0.48256</td></tr><tr><td>agent2/actor_grad_norm</td><td>0.07765</td></tr><tr><td>agent2/critic_grad_norm</td><td>0.03391</td></tr><tr><td>agent2/dist_entropy</td><td>3.6559</td></tr><tr><td>agent2/policy_loss</td><td>-0.06012</td></tr><tr><td>agent2/ratio</td><td>0.12797</td></tr><tr><td>agent2/value_loss</td><td>0.44623</td></tr><tr><td>agent3/actor_grad_norm</td><td>0.06683</td></tr><tr><td>agent3/critic_grad_norm</td><td>0.00701</td></tr><tr><td>agent3/dist_entropy</td><td>3.66688</td></tr><tr><td>agent3/policy_loss</td><td>-0.06782</td></tr><tr><td>agent3/ratio</td><td>0.1328</td></tr><tr><td>agent3/value_loss</td><td>0.4414</td></tr><tr><td>agent4/actor_grad_norm</td><td>0.0507</td></tr><tr><td>agent4/critic_grad_norm</td><td>0.00725</td></tr><tr><td>agent4/dist_entropy</td><td>3.65598</td></tr><tr><td>agent4/policy_loss</td><td>-0.0567</td></tr><tr><td>agent4/ratio</td><td>0.13446</td></tr><tr><td>agent4/value_loss</td><td>0.44968</td></tr><tr><td>agent5/actor_grad_norm</td><td>0.06732</td></tr><tr><td>agent5/critic_grad_norm</td><td>0.03239</td></tr><tr><td>agent5/dist_entropy</td><td>3.66549</td></tr><tr><td>agent5/policy_loss</td><td>-0.06006</td></tr><tr><td>agent5/ratio</td><td>0.13376</td></tr><tr><td>agent5/value_loss</td><td>0.4814</td></tr><tr><td>average_episode_rewards</td><td>261068.53027</td></tr><tr><td>avg_idleness</td><td>69.22325</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rmappo-sepBuffersSkipAsyncAmadmGAE-20230831-234145-seed1</strong> at: <a href='https://wandb.ai/anthony-goeckner/Patrolling/runs/byj63bqf' target=\"_blank\">https://wandb.ai/anthony-goeckner/Patrolling/runs/byj63bqf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 18 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./onpolicy/scripts/results/Patrolling/cumberland/rmappo/sepBuffersSkipAsyncAmadmGAE/wandb/run-20230831_234146-byj63bqf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main([], parsed_args = all_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patrolling_zoo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
