{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic code which shows what it's like to run PPO on the Pistonball env using the parallel API, this code is inspired by CleanRL.\n",
    "\n",
    "This code is exceedingly basic, with no logging or weights saving.\n",
    "The intention was for users to have a (relatively clean) ~200 line file to refer to when they want to design their own learning algorithm.\n",
    "\n",
    "Author: Jet (https://github.com/jjshoots)\n",
    "\n",
    "\n",
    "Modified by Anthony Goeckner for the patrolling zoo environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "from patrolling_zoo.patrolling_zoo_v0 import parallel_env, PatrolGraph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ALGO PARAMS\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ent_coef = 0.1\n",
    "vf_coef = 0.1\n",
    "clip_coef = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 1\n",
    "stack_size = 4\n",
    "frame_size = (64, 64)\n",
    "max_cycles = 500\n",
    "total_episodes = 5\n",
    "\n",
    "\"\"\" ENV SETUP \"\"\"\n",
    "patrolGraph = PatrolGraph(\"patrolling_zoo/env/cumberland.graph\")\n",
    "env = parallel_env(patrolGraph, 3,\n",
    "    require_explicit_visit=True,\n",
    "    max_steps=max_cycles\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "    def __init__(self, num_actions, num_agents, observation_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            # self._layer_init(nn.Conv2d(4, 32, 3, padding=1)),\n",
    "            # nn.MaxPool2d(2),\n",
    "            # nn.ReLU(),\n",
    "            # self._layer_init(nn.Conv2d(32, 64, 3, padding=1)),\n",
    "            # nn.MaxPool2d(2),\n",
    "            # nn.ReLU(),\n",
    "            # self._layer_init(nn.Conv2d(64, 128, 3, padding=1)),\n",
    "            # nn.MaxPool2d(2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Flatten(),\n",
    "            self._layer_init(nn.Linear(observation_size, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
    "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        total_probs =Categorical(logits=logits)\n",
    "        #split the (3,40) logits into 3 (1,40) logits for every agents\n",
    "        splits_logits = torch.split(logits, split_size_or_sections=1, dim=0)\n",
    "        if action is None:\n",
    "            action = []\n",
    "            logprobs = []\n",
    "            entropy = []\n",
    "            for i in range(self.num_agents):\n",
    "                split_logits = splits_logits[i]\n",
    "                probs = Categorical(logits= split_logits)\n",
    "                temp = probs.sample()\n",
    "                action.append(temp.item())\n",
    "                logprobs.append(probs.log_prob(temp).item())\n",
    "                entropy.append(probs.entropy().item())\n",
    "            logprobs = torch.tensor(logprobs)\n",
    "            action = torch.tensor(action)\n",
    "            entropy = torch.tensor(entropy)\n",
    "            return action, logprobs, entropy, self.critic(hidden)\n",
    "        return action, total_probs.log_prob(action), total_probs.entropy(), self.critic(hidden)\n",
    "\n",
    "\n",
    "def batchify_obs(obs_space, obs, device):\n",
    "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "\n",
    "    #np.stack is a method that concencate the tensors along the new axis\n",
    "    obs = np.stack([flatten(obs_space, obs[a]) for a in obs], axis=0)\n",
    "    # convert to torch\n",
    "    obs = torch.tensor(obs).to(device)\n",
    "\n",
    "    return obs\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, env):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = len(env.possible_agents)\n",
    "num_actions = env.action_space(env.possible_agents[0]).n\n",
    "observation_size = flatten_space(env.observation_space(env.possible_agents[0])).shape[0]\n",
    "\n",
    "\"\"\" LEARNER SETUP \"\"\"\n",
    "learner = Learner(num_actions=num_actions, num_agents=num_agents, observation_size=observation_size).to(device)\n",
    "optimizer = optim.Adam(learner.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "\"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "end_step = 0\n",
    "total_episodic_return = 0\n",
    "rb_obs = torch.zeros((max_cycles, num_agents, observation_size)).to(device)\n",
    "rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_values = torch.zeros((max_cycles, num_agents)).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for n number of episodes\n",
    "for episode in range(total_episodes):\n",
    "    # collect an episode\n",
    "    with torch.no_grad():\n",
    "        # collect observations and convert to batch of torch tensors\n",
    "        next_obs = env.reset(seed=None)\n",
    "\n",
    "        # reset the episodic return\n",
    "        total_episodic_return = 0\n",
    "\n",
    "        # each episode has num_steps\n",
    "        for step in range(0, max_cycles):\n",
    "            # rollover the observation\n",
    "            \n",
    "            obs = batchify_obs(env.observation_space(env.possible_agents[0]), next_obs, device)\n",
    "\n",
    "            actions, logprobs, _, values = learner.get_action_and_value(obs)\n",
    "\n",
    "            # execute the environment and log data\n",
    "            next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                unbatchify(actions, env)\n",
    "            )\n",
    "\n",
    "            # add to episode storage\n",
    "            rb_obs[step] = torch.reshape(obs, (num_agents, observation_size))\n",
    "            rb_rewards[step] = batchify(rewards, device)\n",
    "            rb_terms[step] = batchify(terms, device)\n",
    "            rb_actions[step] = actions\n",
    "            rb_logprobs[step] = logprobs\n",
    "            rb_values[step] = values.flatten()\n",
    "\n",
    "            # compute episodic return\n",
    "            total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "            # if we reach termination or truncation, end\n",
    "            if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                end_step = step\n",
    "                break\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "        for t in reversed(range(end_step)):\n",
    "            delta = (\n",
    "                rb_rewards[t]\n",
    "                + gamma * rb_values[t + 1] * rb_terms[t + 1]\n",
    "                - rb_values[t]\n",
    "            )\n",
    "            rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]\n",
    "        rb_returns = rb_advantages + rb_values\n",
    "\n",
    "    # convert our episodes to batch of individual transitions\n",
    "    b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "    b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "    b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "    b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_index = np.arange(len(b_obs))\n",
    "    clip_fracs = []\n",
    "    for repeat in range(3):\n",
    "        # shuffle the indices we use to access the data\n",
    "        np.random.shuffle(b_index)\n",
    "        for start in range(0, len(b_obs), batch_size):\n",
    "            # select the indices we want to train on\n",
    "            end = start + batch_size\n",
    "            batch_index = b_index[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, value = learner.get_action_and_value(\n",
    "                b_obs[batch_index], b_actions.long()[batch_index]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[batch_index]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fracs += [\n",
    "                    ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                ]\n",
    "\n",
    "            # normalize advantaegs\n",
    "            advantages = b_advantages[batch_index]\n",
    "            advantages = (advantages - advantages.mean()) / (\n",
    "                advantages.std() + 1e-8\n",
    "            )\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "            pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                ratio, 1 - clip_coef, 1 + clip_coef\n",
    "            )\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value = value.flatten()\n",
    "            v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "            v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                value - b_values[batch_index],\n",
    "                -clip_coef,\n",
    "                clip_coef,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training episode {episode}\")\n",
    "print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "print(f\"Episode Length: {end_step}\")\n",
    "print(\"\")\n",
    "print(f\"Value Loss: {v_loss.item()}\")\n",
    "print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "print(f\"Approx KL: {approx_kl.item()}\")\n",
    "print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "print(f\"Explained Variance: {explained_var.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.eval()\n",
    "\n",
    "# TEMP: Only run for 30 steps.\n",
    "env.maxSteps = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    # render 2 episodes out\n",
    "    for episode in range(2):\n",
    "        obs = env.reset(seed=None)\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        obs = batchify_obs(env.observation_space(env.possible_agents[0]), obs, device)\n",
    "        terms = [False]\n",
    "        truncs = [False]\n",
    "        while not any(terms) and not any(truncs):\n",
    "            actions, logprobs, _, values = learner.get_action_and_value(obs)\n",
    "            obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))\n",
    "            obs = batchify_obs(env.observation_space(env.possible_agents[0]), obs, device)\n",
    "            terms = [terms[a] for a in terms]\n",
    "            truncs = [truncs[a] for a in truncs]\n",
    "            clear_output(wait=True)\n",
    "            env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
