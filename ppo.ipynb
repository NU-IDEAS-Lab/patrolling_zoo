{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from graph_patrol_env import GraphPatrolEnv\n",
    "from patrol_graph import PatrolGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy_network,\n",
    "        policy_lr,\n",
    "        value_lr,\n",
    "        gamma,\n",
    "        gae_lambda,\n",
    "        entropy_coef,\n",
    "        value_clip,\n",
    "        num_epochs,\n",
    "        num_mini_batches,\n",
    "        ppo_ratio_clip,\n",
    "        max_grad_norm,\n",
    "        device=torch.device(\"cpu\")\n",
    "    ):\n",
    "        self.policy_network = policy_network.to(device)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=policy_lr)\n",
    "        self.value_network = policy_network.to(device)\n",
    "        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=value_lr)\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_clip = value_clip\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_mini_batches = num_mini_batches\n",
    "        self.ppo_ratio_clip = ppo_ratio_clip\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.device = device\n",
    "        self.episode_buffer = []\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def act(self, obs):\n",
    "        actions = {}\n",
    "        with torch.no_grad():\n",
    "            for agent, agent_obs in obs.items():\n",
    "                obs_tensor = torch.tensor(agent_obs, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                action_probs = self.policy_network(obs_tensor)\n",
    "                action_probs_np = action_probs.cpu().numpy()\n",
    "                action = np.random.choice(len(action_probs_np), p=action_probs_np)\n",
    "                actions[agent] = action\n",
    "        return actions\n",
    "\n",
    "    def collect(self, obs, actions, rewards, next_obs, dones):\n",
    "        self.episode_buffer.append((obs, actions, rewards, next_obs, dones))\n",
    "\n",
    "    def train(self):\n",
    "        obs, actions, rewards, next_obs, dones = zip(*self.episode_buffer)\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "        actions_tensor = torch.tensor(actions).to(self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        next_obs_tensor = torch.tensor(next_obs, dtype=torch.float32).to(self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        self.episode_buffer = []\n",
    "\n",
    "        values = self.value_network(obs_tensor).squeeze()\n",
    "        next_values = self.value_network(next_obs_tensor).squeeze()\n",
    "        advantages = self.calculate_advantages(rewards_tensor, dones_tensor, values, next_values)\n",
    "\n",
    "        for _ in range(self.num_epochs):\n",
    "            indices = np.arange(len(obs))\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for i in range(0, len(indices), self.num_mini_batches):\n",
    "                batch_indices = indices[i : i + self.num_mini_batches]\n",
    "                obs_batch = obs_tensor[batch_indices]\n",
    "                actions_batch = actions_tensor[batch_indices]\n",
    "                advantages_batch = advantages[batch_indices]\n",
    "                old_action_probs = self.policy_network(obs_batch).gather(1, actions_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "                for _ in range(3):  # PPO optimization loop\n",
    "                    action_probs = self.policy_network(obs_batch)\n",
    "                    ratio = action_probs / (old_action_probs + 1e-5)\n",
    "                    surr1 = ratio * advantages_batch.unsqueeze(1)\n",
    "                    surr2 = torch.clamp(ratio, 1.0 - self.ppo_ratio_clip, 1.0 + self.ppo_ratio_clip) * advantages_batch.unsqueeze(1)\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                    values_batch = self.value_network(obs_batch).squeeze()\n",
    "                    clipped_values = values_batch + torch.clamp(values_batch - values[batch_indices], -self.value_clip, self.value_clip)\n",
    "                    value_loss = 0.5 * torch.max((values_batch - rewards_tensor[batch_indices]) ** 2, (clipped_values - rewards_tensor[batch_indices]) ** 2).mean()\n",
    "\n",
    "                    entropy_loss = -(action_probs * torch.log(action_probs + 1e-5)).sum(dim=1).mean()\n",
    "\n",
    "                    self.policy_optimizer.zero_grad()\n",
    "                    policy_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.policy_network.parameters(), self.max_grad_norm)\n",
    "                    self.policy_optimizer.step()\n",
    "\n",
    "                    self.value_optimizer.zero_grad()\n",
    "                    value_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.value_network.parameters(), self.max_grad_norm)\n",
    "                    self.value_optimizer.step()\n",
    "\n",
    "        self.total_steps += len(obs)\n",
    "\n",
    "    def calculate_advantages(self, rewards, dones, values, next_values):\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        next_advantage = 0\n",
    "        next_non_terminal = 1 - dones[-1]\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = rewards[i] + self.gamma * next_non_terminal * next_values[i] - values[i]\n",
    "            advantage = delta + self.gamma * self.gae_lambda * advantage\n",
    "            advantages.append(advantage)\n",
    "            next_non_terminal = 1 - dones[i]\n",
    "            next_advantage = advantage\n",
    "        advantages.reverse()\n",
    "        return torch.tensor(advantages, dtype=torch.float32).to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Instantiate the PPO agent\n",
    "env = GraphPatrolEnv(PatrolGraph(\"cumberland.graph\"), 3)\n",
    "input_dim = len(env.pg.graph)\n",
    "output_dim = len(env.pg.graph)\n",
    "policy_net = PolicyNetwork(input_dim, output_dim)\n",
    "agent = PPOAgent(\n",
    "    policy_net,\n",
    "    policy_lr=0.001,\n",
    "    value_lr=0.001,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    entropy_coef=0.01,\n",
    "    value_clip=True,\n",
    "    num_epochs=4,\n",
    "    num_mini_batches=32,\n",
    "    ppo_ratio_clip=0.2,\n",
    "    max_grad_norm=0.5,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 0D and 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m episode_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m----> 9\u001b[0m     actions \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mact(obs)\n\u001b[1;32m     10\u001b[0m     next_obs, rewards, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[1;32m     11\u001b[0m     agent\u001b[39m.\u001b[39mcollect(obs, actions, rewards, next_obs, done)\n",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36mPPOAgent.act\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m agent, agent_obs \u001b[39min\u001b[39;00m obs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     43\u001b[0m     obs_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(agent_obs, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 44\u001b[0m     action_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_network(obs_tensor)\n\u001b[1;32m     45\u001b[0m     action_probs_np \u001b[39m=\u001b[39m action_probs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     46\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mlen\u001b[39m(action_probs_np), p\u001b[39m=\u001b[39maction_probs_np)\n",
      "File \u001b[0;32m~/miniconda3/envs/pettingzoo2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x))\n\u001b[1;32m     10\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/pettingzoo2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pettingzoo2/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 0D and 2D"
     ]
    }
   ],
   "source": [
    "# Step 3: Train the agent\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        actions = agent.act(obs)\n",
    "        next_obs, rewards, done, _ = env.step(actions)\n",
    "        agent.collect(obs, actions, rewards, next_obs, done)\n",
    "        obs = next_obs\n",
    "        episode_reward += sum(rewards.values())\n",
    "\n",
    "    agent.train()\n",
    "\n",
    "    # Print episode information\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} - Reward: {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Use the trained agent\n",
    "# You can use the trained agent to act in the environment and evaluate its performance.\n",
    "# For example:\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    actions = agent.act(obs)\n",
    "    obs, rewards, done, _ = env.step(actions)\n",
    "    env.plot_world()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
